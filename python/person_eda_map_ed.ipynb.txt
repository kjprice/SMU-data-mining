{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "The United States constitution had made mandatory the decennial census.   The census provided a headcount which did not provide enough details to make data driven decisions about the use of government funds.  A long form with necessary details was introduced in the 20th century.   Only one in six households received the additional questions on the long form.  In the 21st century, the American Community Survey (ACS) was established to conduct ongoing surveys with the details previously included on the decennial long form census.   The ACS survey results provide data to determine the appropriate distribution of more than $400 billion in federal and state funds.  The funds are for social programs as well as physical and service infrastructure.  \n",
    "\n",
    "Annually 3.5 million homes are surveyed.   A random sample was performed on both the housing data as well as the personal data to allow for manageable data sets while including a macro vision of the data.  We will consider the data the training sets.\n",
    "The validation of the data can be done in several ways.  A statistically sound method would be to validate the data using another random sampling which would be the test set.   Another potential option would be the validate the federal spending.  Due to the difficulty in finding the specifics spending details, the spending validation model would be prohibited by the time constraints as well as data availability constraints. \n",
    "\n",
    "Source:  https://www.census.gov/programs-surveys/acs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "The focus of the exploratory data analysis is the personal data.  The personal data contains 283 attributes.  The data dictionary link below provides the description of the attributes.  \n",
    "\n",
    "Data Dictionary:  https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Polygon, PathPatch\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "fig.suptitle('PUMA Shapefiles: An Example with Basemap', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a map object with the Albert Equal Areas projection.\n",
    "# This projection tends to look nice for the contiguous us.\n",
    "m = Basemap(width=5000000,height=3500000,\n",
    "            resolution='l',projection='aea',\\\n",
    "            lat_1=30.,lat_2=50,lon_0=-96,lat_0=38)\n",
    "            \n",
    "# state codes from http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt\n",
    "# note that areas outside of the conus have been commented out\n",
    "state_codes = {\n",
    "    '01': 'Alabama',\n",
    "    #'02': 'Alaska',\n",
    "    '04': 'Arizona',\n",
    "    '05': 'Arkansas',\n",
    "    '06': 'California',\n",
    "    '08': 'Colorado',\n",
    "    '09': 'Connecticut',\n",
    "    '10': 'Delaware',\n",
    "    '11': 'District of Columbia',\n",
    "    '12': 'Florida',\n",
    "    '13': 'Georgia',\n",
    "    '15': 'Hawaii',\n",
    "    '16': 'Idaho',\n",
    "    '17': 'Illinois',\n",
    "    '18': 'Indiana',\n",
    "    '19': 'Iowa',\n",
    "    '20': 'Kansas',\n",
    "    '21': 'Kentucky',\n",
    "    '22': 'Louisiana',\n",
    "    '23': 'Maine',\n",
    "    '24': 'Maryland',\n",
    "    '25': 'Massachusetts',\n",
    "    '26': 'Michigan',\n",
    "    '27': 'Minnesota',\n",
    "    '28': 'Mississippi',\n",
    "    '29': 'Missouri',\n",
    "    '30': 'Montana',\n",
    "    '31': 'Nebraska',\n",
    "    '32': 'Nevada',\n",
    "    '33': 'New Hampshire',\n",
    "    '34': 'New Jersey',\n",
    "    '35': 'New Mexico',\n",
    "    '36': 'New York',\n",
    "    '37': 'North Carolina',\n",
    "    '38': 'North Dakota',\n",
    "    '39': 'Ohio',\n",
    "    '40': 'Oklahoma',\n",
    "    '41': 'Oregon',\n",
    "    '42': 'Pennsylvania',\n",
    "    '44': 'Rhode Island',\n",
    "    '45': 'South Carolina',\n",
    "    '46': 'South Dakota',\n",
    "    '47': 'Tennessee',\n",
    "    '48': 'Texas',\n",
    "    '49': 'Utah',\n",
    "    '50': 'Vermont',\n",
    "    '51': 'Virginia',\n",
    "    '53': 'Washington',\n",
    "    '54': 'West Virginia',\n",
    "    '55': 'Wisconsin',\n",
    "    '56': 'Wyoming',\n",
    "    #'72': 'Puerto Rico'\n",
    "}       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a colorramp\n",
    "num_colors = 12\n",
    "cm = plt.get_cmap('Blues')\n",
    "blues = [cm(1.*i/num_colors) for i in range(num_colors)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add colorbar legend\n",
    "cmap = mpl.colors.ListedColormap(blues)\n",
    "# define the bins\n",
    "bounds = np.linspace(0.0, 1.0, num_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset contained approximately 3.1 million records. We quickly found that using the entire dataset would be much too arduous on our computers and would take too much time to gain meaningful information from the data.\n",
    "\n",
    "Instead of using the massive, original dataset; we decided to take a subset of the dataset. The dataset we created includes 2.5% of the original observations (approximatly 78,000 rows). We recognize that there will be some loss in the interpretability of any statistical observations we make on this subset.  We use groupings of appropriate levels within the features e.i. poverty by state to gain a general sense of the wealth across the contenintal US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/person/person-subset-2.5percent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poverty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['logPINCP'] = np.log(df.PINCP+(-1*df.PINCP.min())+1)\n",
    "df.POVPIP.describe()\n",
    "df['PCNIProlog'] = df['logPINCP'].fillna(0).astype(np.int64)\n",
    "print(df.logPINCP.min())\n",
    "print(df.logPINCP[1])\n",
    "print(df.logPINCP.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.POVPIP.hist()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grab PUMA\n",
    "#puma = df.groupby(['PUMA'])\n",
    "#puma.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#puma.PCNIProlog.mean().div(df.PCNIProlog.max())\n",
    "#puma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# read each states shapefile\n",
    "for key in state_codes.keys():\n",
    "    m.readshapefile('/data/shapefiles/tl_2013_{0}_puma10'.format(key),\n",
    "                    name='state', drawbounds=True, default_encoding='latin-1')\n",
    "                    \n",
    "    # loop through each PUMA and assign a color from our colormap based on average collection of log income for given puma\n",
    "    for info, shape in zip(m.state_info, m.state):\n",
    "        #print(np.int64(np.rint(df.PCNIProlog[df['PUMA'].isin([info['PUMACE10']]) & (df['ST'].isin([key]))].mean())))\n",
    "        #print(puma.PCNIProlog[puma.PUMA==info['PUMACE10']])\n",
    "        patches = [Polygon(np.array(shape), True)]\n",
    "        pc = PatchCollection(patches, edgecolor='k', linewidths=1., zorder=2)\n",
    "        pc.set_color(blues[np.int64(np.rint(df.PCNIProlog[df['PUMA'].isin([info['PUMACE10']]) & (df['ST'].isin([key]))].mean()))])\n",
    "        #pc.set_color(random.choice(blues))\n",
    "        ax.add_collection(pc)\n",
    "\n",
    "print(blues)\n",
    "# create a second axes for the colorbar\n",
    "ax2 = fig.add_axes([0.82, 0.1, 0.03, 0.8])\n",
    "cb = mpl.colorbar.ColorbarBase(ax2, cmap=cmap, ticks=bounds, boundaries=bounds,\n",
    "                               format='%1i')\n",
    "cb.ax.set_yticklabels([str(round(i, 2)) for i in bounds])\n",
    "\n",
    "plt.savefig('/data/gen/person_conus.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Data Sources\n",
    "\n",
    "We use a subset of the kaggle data source that came from the 2013 American Community Survey (ACS).  The ACS website has more information on the weights and replications used. \n",
    "\n",
    "We also use a map found from another kaggle site and again supplemented the shape files with the ones available at the ACS website.\n",
    "\n",
    "The income data was also supplemented with data from another site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
