{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We continue our work, with census data, from [Project 1](https://gist.github.com/kjprice/820c75bd8e5c3f2558f4576f38893dae), to take a deeper look into our data. We move beyond exploratory data analysis and will now look into classifying the data based on the given attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load in raw dataset\n",
    "person_raw = pd.read_csv('../data/person-subset-2.5percent.csv')\n",
    "\n",
    "# clean data (as performed in Project 1)\n",
    "# will provide us with a new dataset \"df\"\n",
    "# ...and a list of \"important_features\"\n",
    "execfile('../python/clean_data_person.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the `important_features` discovered from the previous project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60357 entries, 0 to 78317\n",
      "Data columns (total 14 columns):\n",
      "PINCP     60357 non-null float64\n",
      "POVPIP    57892 non-null float64\n",
      "JWMNP     32486 non-null float64\n",
      "AGEP      60357 non-null int64\n",
      "PWGTP     60357 non-null int64\n",
      "PAP       60357 non-null float64\n",
      "CIT       60357 non-null object\n",
      "ENG       60357 non-null object\n",
      "COW       60357 non-null object\n",
      "PUMA      60357 non-null category\n",
      "SEX       60357 non-null object\n",
      "MIL       60357 non-null object\n",
      "SCHL      60357 non-null float64\n",
      "MAR       60357 non-null object\n",
      "dtypes: category(1), float64(5), int64(2), object(6)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df[important_features].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Categorical Variable\n",
    "\n",
    "Along with these attributes above, as defined in our [previous project](https://gist.github.com/kjprice/820c75bd8e5c3f2558f4576f38893dae), we will want to add another variable which we will use to perform a classification analysis on. This variable should be categorical and would, ideally, continue on with our theme of \"predicting income\". Income (`PINCP`), as we have it currently, is a continuous variable. We will take income and will create a new categorical variable called `affluency`, which will take on the values \"general\" and \"rich\" based on whether the individual makes less (or more) than $100,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_affluency():\n",
    "    global lr\n",
    "    global important_features\n",
    "\n",
    "    lr = df[important_features].copy(deep=True)\n",
    "    lr['affluency'] = pd.cut(df.PINCP, [-1, 99999.99, 1e12], labels=('general', 'rich'))\n",
    "create_affluency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Now that we have our new categorical variable, and a new dataset (`lr`) based on our `important_features`, let's try to clean up our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will remove unwanted fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lr['POVPIP']\n",
    "del lr['PINCP']\n",
    "del lr['PUMA']\n",
    "del lr['MIL']\n",
    "del lr['MAR']\n",
    "del lr['SCHL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we group \"Travel Time\" (`JWMNP`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_travel_time():\n",
    "    lr.JWMNP = lr.JWMNP.fillna(-1)\n",
    "    lr['travel_time'] = pd.cut(lr.JWMNP, (-2, 0, 15, 40, 60, lr.JWMNP.max()), labels=['na', 'short', 'half hour', 'hour', 'long'])\n",
    "    del lr['JWMNP']\n",
    "calculate_travel_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, from our variables `affluency` and `SEX`, we will create the boolean variables `wealthy` and `is_male` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bools():\n",
    "    lr['wealthy'] = lr.affluency == 'rich'\n",
    "    del lr['affluency']\n",
    "    lr['is_male'] = lr.SEX == 'Male'\n",
    "    lr.is_male = lr.is_male.astype(np.int)\n",
    "    del lr['SEX']\n",
    "calculate_bools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform one-hot-encoding on our other categorical variables `travel_time`, `CIT`, `ENG`, `COW`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode():\n",
    "    global lr\n",
    "    one_hot_travel_time = pd.get_dummies(lr.travel_time, prefix='Travel_Time_')\n",
    "    lr = pd.concat((lr, one_hot_travel_time), axis=1)\n",
    "    del lr['travel_time']\n",
    "\n",
    "    one_hot_citizenship = pd.get_dummies(lr.CIT, prefix='Citizen_')\n",
    "    lr = pd.concat((lr, one_hot_citizenship), axis=1)\n",
    "    del lr['CIT']\n",
    "\n",
    "    one_hot_english = pd.get_dummies(lr.ENG, prefix='English_')\n",
    "    lr = pd.concat((lr, one_hot_english), axis=1)\n",
    "    del lr['ENG']\n",
    "\n",
    "    one_hot_worker_class = pd.get_dummies(lr.COW, prefix='Worker_Class_')\n",
    "    lr = pd.concat((lr, one_hot_worker_class), axis=1)\n",
    "    del lr['COW']\n",
    "one_hot_encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our dataset looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60357 entries, 0 to 78317\n",
      "Data columns (total 30 columns):\n",
      "AGEP                                               60357 non-null int64\n",
      "PWGTP                                              60357 non-null int64\n",
      "PAP                                                60357 non-null float64\n",
      "wealthy                                            60357 non-null bool\n",
      "is_male                                            60357 non-null int64\n",
      "Travel_Time__half hour                             60357 non-null uint8\n",
      "Travel_Time__hour                                  60357 non-null uint8\n",
      "Travel_Time__long                                  60357 non-null uint8\n",
      "Travel_Time__na                                    60357 non-null uint8\n",
      "Travel_Time__short                                 60357 non-null uint8\n",
      "Citizen__Born Abroad)                              60357 non-null uint8\n",
      "Citizen__Naturalized                               60357 non-null uint8\n",
      "Citizen__Non-Citizen                               60357 non-null uint8\n",
      "Citizen__US Born                                   60357 non-null uint8\n",
      "Citizen__US Territory Born                         60357 non-null uint8\n",
      "English__Not at all                                60357 non-null uint8\n",
      "English__Not well                                  60357 non-null uint8\n",
      "English__Speaks only English                       60357 non-null uint8\n",
      "English__Very well                                 60357 non-null uint8\n",
      "English__Well                                      60357 non-null uint8\n",
      "Worker_Class__Family Business - no pay             60357 non-null uint8\n",
      "Worker_Class__Federal Government                   60357 non-null uint8\n",
      "Worker_Class__Local Government                     60357 non-null uint8\n",
      "Worker_Class__Private Non-Profit                   60357 non-null uint8\n",
      "Worker_Class__Private for Profit                   60357 non-null uint8\n",
      "Worker_Class__Self Employed (incorporated)         60357 non-null uint8\n",
      "Worker_Class__Self Employed (not incorportated)    60357 non-null uint8\n",
      "Worker_Class__State Government                     60357 non-null uint8\n",
      "Worker_Class__Unemployeed                          60357 non-null uint8\n",
      "Worker_Class__nan                                  60357 non-null uint8\n",
      "dtypes: bool(1), float64(1), int64(3), uint8(25)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "lr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we now we have numeric fields to work with. now we can begin our analysis..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can create our response `y` and explanatory `X` variables. Then we standardize `X` values. Finally, we split the dataset into a 20/80 testing/training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_for_analysis():\n",
    "   global lr2\n",
    "   ### Create reponse and explanatory variables\n",
    "   lr2 = lr.copy(deep=True)\n",
    "   y = lr2.wealthy.values\n",
    "   del lr2['wealthy']\n",
    "   X = lr2.values\n",
    "\n",
    "   ### Standardize X values\n",
    "   scl_obj = StandardScaler()\n",
    "   scl_obj.fit(X)\n",
    "   X = scl_obj.transform(X)\n",
    "   \n",
    "   ### Get training/test data\n",
    "   return train_test_split(X, y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = clean_data_for_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function will analyze our data using a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.692842942346 \n",
      "\n",
      "CPU times: user 424 ms, sys: 19.4 ms, total: 443 ms\n",
      "Wall time: 439 ms\n"
     ]
    }
   ],
   "source": [
    "def run_logistic_regression():\n",
    "   ### Create reusable logistic regression object\n",
    "   global lr_clf\n",
    "   lr_clf = LogisticRegression(penalty='l2', C=0.05, class_weight='balanced', n_jobs=-1)\n",
    "   \n",
    "   ### Create and test accuracy of our model\n",
    "   lr_clf.fit(X_train, y_train)\n",
    "   y_hat = lr_clf.predict(X_test)\n",
    "   \n",
    "   acc = mt.accuracy_score(y_test, y_hat)\n",
    "   \n",
    "   print('accuracy: %s \\n' % acc)\n",
    "\n",
    "%time run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.666832339298 \n",
      "\n",
      "CPU times: user 238 ms, sys: 24 ms, total: 262 ms\n",
      "Wall time: 76.7 ms\n"
     ]
    }
   ],
   "source": [
    "def run_sgd():\n",
    "   regularize_const = 0.1\n",
    "   iterations = 5\n",
    "   svm_sgd = SGDClassifier(alpha=regularize_const, class_weight='balanced',\n",
    "           fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "           loss='hinge', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "   \n",
    "   svm_sgd.fit(X_train, y_train)\n",
    "   y_hat = svm_sgd.predict(X_test)\n",
    "   \n",
    "   acc = mt.accuracy_score(y_test, y_hat)\n",
    "   conf = mt.confusion_matrix(y_test, y_hat)\n",
    "   \n",
    "   print('accuracy: %s \\n' % acc)\n",
    "\n",
    "%time run_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can try to change our variables to see if we get a different score. Let's see how eduction (`SCHL`), marital status (`MAR`), and military service (`MIL`) affect our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_affluency()\n",
    "calculate_travel_time()\n",
    "calculate_bools()\n",
    "one_hot_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lr['POVPIP']\n",
    "del lr['PINCP']\n",
    "del lr['PUMA']\n",
    "\n",
    "### Group \"education attained\"\n",
    "lr.SCHL = lr.SCHL.fillna(-1)\n",
    "lr['education_attained'] = pd.cut(lr.SCHL, (-2, 1, 11, 15, 17, 20, 21, 22, 23, 24), labels=['na', 'no_high_school', 'some_high_school', 'HS_diploma', 'some_college', 'bacholors_degree', 'masters_degree', 'professional_degree', 'doctorate_degree'])\n",
    "del lr['SCHL']\n",
    "\n",
    "\n",
    "### One-hot-encode our new categorical variables\n",
    "one_hot_military = pd.get_dummies(lr.MIL, prefix='Miliary_Service_')\n",
    "lr = pd.concat((lr, one_hot_military), axis=1)\n",
    "del lr['MIL']\n",
    "\n",
    "one_hot_education_attained = pd.get_dummies(lr.education_attained, prefix='Education_Attained_')\n",
    "lr = pd.concat((lr, one_hot_education_attained), axis=1)\n",
    "del lr['education_attained']\n",
    "\n",
    "one_hot_marital_status = pd.get_dummies(lr.MAR, prefix=\"Marital_status_\")\n",
    "lr = pd.concat((lr, one_hot_marital_status), axis=1)\n",
    "del lr['MAR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression:\n",
      "accuracy: 0.784459907223 \n",
      "\n",
      "CPU times: user 602 ms, sys: 34.6 ms, total: 637 ms\n",
      "Wall time: 645 ms\n",
      "\n",
      "\n",
      "support vector machine:\n",
      "accuracy: 0.776838966203 \n",
      "\n",
      "CPU times: user 310 ms, sys: 23.5 ms, total: 333 ms\n",
      "Wall time: 86.5 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = clean_data_for_analysis()\n",
    "\n",
    "print('logistic regression:')\n",
    "%time run_logistic_regression()\n",
    "\n",
    "print('\\n\\nsupport vector machine:')\n",
    "%time run_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see that our accuracy greatly approves with the addition of these several variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages between algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that logistic regression gets a better accuracy (about 1%) than support vector machines (using stochastic gradient descent). Logistic regression takes about four times longer, however, than support vector machines. Using a batch gradient descent gives support vectors additional accuracy (consistent with logistic regression) but takes abou three minutes to complete (approximately 250 times slower than logistic regression)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
