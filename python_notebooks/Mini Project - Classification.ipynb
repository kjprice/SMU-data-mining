{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We continue our work, with census data, from [Project 1](https://gist.github.com/kjprice/820c75bd8e5c3f2558f4576f38893dae), to take a deeper look into our data. We move beyond exploratory data analysis and will now look into classifying the data based on the given attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load in raw dataset\n",
    "person_raw = pd.read_csv('../data/person-subset-2.5percent.csv')\n",
    "\n",
    "# clean data (as performed in Project 1)\n",
    "# will provide us with a new dataset \"df\"\n",
    "# ...and a list of \"important_features\"\n",
    "execfile('../python/clean_data_person.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the `important_features` discovered from the previous project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60357 entries, 0 to 78317\n",
      "Data columns (total 14 columns):\n",
      "PINCP     60357 non-null float64\n",
      "POVPIP    57892 non-null float64\n",
      "JWMNP     32486 non-null float64\n",
      "AGEP      60357 non-null int64\n",
      "PWGTP     60357 non-null int64\n",
      "PAP       60357 non-null float64\n",
      "CIT       60357 non-null object\n",
      "ENG       60357 non-null object\n",
      "COW       60357 non-null object\n",
      "PUMA      60357 non-null category\n",
      "SEX       60357 non-null object\n",
      "MIL       60357 non-null object\n",
      "SCHL      60357 non-null float64\n",
      "MAR       60357 non-null object\n",
      "dtypes: category(1), float64(5), int64(2), object(6)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df[important_features].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Categorical Variable\n",
    "\n",
    "Along with these attributes above, as defined in our [previous project](https://gist.github.com/kjprice/820c75bd8e5c3f2558f4576f38893dae), we will want to add another variable which we will use to perform a classification analysis on. This variable should be categorical and would, ideally, continue on with our theme of \"predicting income\". Income (`PINCP`), as we have it currently, is a continuous variable. We will take income and will create a new categorical variable called `affluency`, which will take on the values \"general\" and \"rich\" based on whether the individual makes less (or more) than $100,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_affluency():\n",
    "    global lr\n",
    "    global important_features\n",
    "\n",
    "    lr = df[important_features].copy(deep=True)\n",
    "    lr['affluency'] = pd.cut(df.PINCP, [-1, 99999.99, 1e12], labels=('general', 'rich'))\n",
    "create_affluency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Now that we have our new categorical variable, and a new dataset (`lr`) based on our `important_features`, let's try to clean up our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will remove unwanted fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del lr['POVPIP']\n",
    "del lr['PINCP']\n",
    "del lr['PUMA']\n",
    "del lr['MIL']\n",
    "del lr['MAR']\n",
    "del lr['SCHL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we group \"Travel Time\" (`JWMNP`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_travel_time():\n",
    "    lr.JWMNP = lr.JWMNP.fillna(-1)\n",
    "    lr['travel_time'] = pd.cut(lr.JWMNP, (-2, 0, 15, 40, 60, lr.JWMNP.max()), labels=['na', 'short', 'half hour', 'hour', 'long'])\n",
    "    del lr['JWMNP']\n",
    "calculate_travel_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, from our variables `affluency` and `SEX`, we will create the boolean variables `wealthy` and `is_male` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_bools():\n",
    "    lr['wealthy'] = lr.affluency == 'rich'\n",
    "    del lr['affluency']\n",
    "    lr['is_male'] = lr.SEX == 'Male'\n",
    "    lr.is_male = lr.is_male.astype(np.int)\n",
    "    del lr['SEX']\n",
    "calculate_bools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can perform one-hot-encoding on our other categorical variables `travel_time`, `CIT`, `ENG`, `COW`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode():\n",
    "    global lr\n",
    "    one_hot_travel_time = pd.get_dummies(lr.travel_time, prefix='Travel_Time_')\n",
    "    lr = pd.concat((lr, one_hot_travel_time), axis=1)\n",
    "    del lr['travel_time']\n",
    "\n",
    "    one_hot_citizenship = pd.get_dummies(lr.CIT, prefix='Citizen_')\n",
    "    lr = pd.concat((lr, one_hot_citizenship), axis=1)\n",
    "    del lr['CIT']\n",
    "\n",
    "    one_hot_english = pd.get_dummies(lr.ENG, prefix='English_')\n",
    "    lr = pd.concat((lr, one_hot_english), axis=1)\n",
    "    del lr['ENG']\n",
    "\n",
    "    one_hot_worker_class = pd.get_dummies(lr.COW, prefix='Worker_Class_')\n",
    "    lr = pd.concat((lr, one_hot_worker_class), axis=1)\n",
    "    del lr['COW']\n",
    "one_hot_encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our dataset looks now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60357 entries, 0 to 78317\n",
      "Data columns (total 30 columns):\n",
      "AGEP                                               60357 non-null int64\n",
      "PWGTP                                              60357 non-null int64\n",
      "PAP                                                60357 non-null float64\n",
      "wealthy                                            60357 non-null bool\n",
      "is_male                                            60357 non-null int64\n",
      "Travel_Time__half hour                             60357 non-null uint8\n",
      "Travel_Time__hour                                  60357 non-null uint8\n",
      "Travel_Time__long                                  60357 non-null uint8\n",
      "Travel_Time__na                                    60357 non-null uint8\n",
      "Travel_Time__short                                 60357 non-null uint8\n",
      "Citizen__Born Abroad)                              60357 non-null uint8\n",
      "Citizen__Naturalized                               60357 non-null uint8\n",
      "Citizen__Non-Citizen                               60357 non-null uint8\n",
      "Citizen__US Born                                   60357 non-null uint8\n",
      "Citizen__US Territory Born                         60357 non-null uint8\n",
      "English__Not at all                                60357 non-null uint8\n",
      "English__Not well                                  60357 non-null uint8\n",
      "English__Speaks only English                       60357 non-null uint8\n",
      "English__Very well                                 60357 non-null uint8\n",
      "English__Well                                      60357 non-null uint8\n",
      "Worker_Class__Family Business - no pay             60357 non-null uint8\n",
      "Worker_Class__Federal Government                   60357 non-null uint8\n",
      "Worker_Class__Local Government                     60357 non-null uint8\n",
      "Worker_Class__Private Non-Profit                   60357 non-null uint8\n",
      "Worker_Class__Private for Profit                   60357 non-null uint8\n",
      "Worker_Class__Self Employed (incorporated)         60357 non-null uint8\n",
      "Worker_Class__Self Employed (not incorportated)    60357 non-null uint8\n",
      "Worker_Class__State Government                     60357 non-null uint8\n",
      "Worker_Class__Unemployeed                          60357 non-null uint8\n",
      "Worker_Class__nan                                  60357 non-null uint8\n",
      "dtypes: bool(1), float64(1), int64(3), uint8(25)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "lr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we now we have numeric fields to work with. now we can begin our analysis..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can create our response `y` and explanatory `X` variables. Then we standardize `X` values. Finally, we split the dataset into a 20/80 testing/training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_for_analysis():\n",
    "   global lr2\n",
    "   ### Create reponse and explanatory variables\n",
    "   lr2 = lr.copy(deep=True)\n",
    "   y = lr2.wealthy.values\n",
    "   del lr2['wealthy']\n",
    "   X = lr2.values\n",
    "\n",
    "   ### Standardize X values\n",
    "   scl_obj = StandardScaler()\n",
    "   scl_obj.fit(X)\n",
    "   X = scl_obj.transform(X)\n",
    "   \n",
    "   ### Get training/test data\n",
    "   return train_test_split(X, y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = clean_data_for_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function will analyze our data using a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.692842942346 \n",
      "\n",
      "CPU times: user 424 ms, sys: 19.4 ms, total: 443 ms\n",
      "Wall time: 439 ms\n"
     ]
    }
   ],
   "source": [
    "def run_logistic_regression():\n",
    "   ### Create reusable logistic regression object\n",
    "   global lr_clf\n",
    "   lr_clf = LogisticRegression(penalty='l2', C=0.05, class_weight='balanced', n_jobs=-1)\n",
    "   \n",
    "   ### Create and test accuracy of our model\n",
    "   lr_clf.fit(X_train, y_train)\n",
    "   y_hat = lr_clf.predict(X_test)\n",
    "   \n",
    "   acc = mt.accuracy_score(y_test, y_hat)\n",
    "   \n",
    "   print('accuracy: %s \\n' % acc)\n",
    "\n",
    "%time run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.666832339298 \n",
      "\n",
      "CPU times: user 238 ms, sys: 24 ms, total: 262 ms\n",
      "Wall time: 76.7 ms\n"
     ]
    }
   ],
   "source": [
    "def run_sgd():\n",
    "   regularize_const = 0.1\n",
    "   iterations = 5\n",
    "   svm_sgd = SGDClassifier(alpha=regularize_const, class_weight='balanced',\n",
    "           fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "           loss='hinge', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "   \n",
    "   svm_sgd.fit(X_train, y_train)\n",
    "   y_hat = svm_sgd.predict(X_test)\n",
    "   \n",
    "   acc = mt.accuracy_score(y_test, y_hat)\n",
    "   conf = mt.confusion_matrix(y_test, y_hat)\n",
    "   \n",
    "   print('accuracy: %s \\n' % acc)\n",
    "\n",
    "%time run_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can try to change our variables to see if we get a different score. Let's see how eduction (`SCHL`), marital status (`MAR`), and military service (`MIL`) affect our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_affluency()\n",
    "calculate_travel_time()\n",
    "calculate_bools()\n",
    "one_hot_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del lr['POVPIP']\n",
    "del lr['PINCP']\n",
    "del lr['PUMA']\n",
    "\n",
    "### Group \"education attained\"\n",
    "lr.SCHL = lr.SCHL.fillna(-1)\n",
    "lr['education_attained'] = pd.cut(lr.SCHL, (-2, 1, 11, 15, 17, 20, 21, 22, 23, 24), labels=['na', 'no_high_school', 'some_high_school', 'HS_diploma', 'some_college', 'bacholors_degree', 'masters_degree', 'professional_degree', 'doctorate_degree'])\n",
    "del lr['SCHL']\n",
    "\n",
    "\n",
    "### One-hot-encode our new categorical variables\n",
    "one_hot_military = pd.get_dummies(lr.MIL, prefix='Miliary_Service_')\n",
    "lr = pd.concat((lr, one_hot_military), axis=1)\n",
    "del lr['MIL']\n",
    "\n",
    "one_hot_education_attained = pd.get_dummies(lr.education_attained, prefix='Education_Attained_')\n",
    "lr = pd.concat((lr, one_hot_education_attained), axis=1)\n",
    "del lr['education_attained']\n",
    "\n",
    "one_hot_marital_status = pd.get_dummies(lr.MAR, prefix=\"Marital_status_\")\n",
    "lr = pd.concat((lr, one_hot_marital_status), axis=1)\n",
    "del lr['MAR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression:\n",
      "accuracy: 0.784459907223 \n",
      "\n",
      "CPU times: user 602 ms, sys: 34.6 ms, total: 637 ms\n",
      "Wall time: 645 ms\n",
      "\n",
      "\n",
      "support vector machine:\n",
      "accuracy: 0.776838966203 \n",
      "\n",
      "CPU times: user 310 ms, sys: 23.5 ms, total: 333 ms\n",
      "Wall time: 86.5 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = clean_data_for_analysis()\n",
    "\n",
    "print('logistic regression:')\n",
    "%time run_logistic_regression()\n",
    "\n",
    "print('\\n\\nsupport vector machine:')\n",
    "%time run_sgd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see that our accuracy greatly approves with the addition of these several variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages between algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that logistic regression gets a better accuracy (about 1%) than support vector machines (using stochastic gradient descent). Logistic regression takes about four times longer, however, than support vector machines. Using a batch gradient descent gives support vectors additional accuracy (consistent with logistic regression) but takes abou three minutes to complete (approximately 250 times slower than logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights of variables\n",
      "('Education_Attained__HS_diploma', 'has weight of', -0.39658104784975245)\n",
      "('Education_Attained__some_high_school', 'has weight of', -0.35910426108900412)\n",
      "('Worker_Class__nan', 'has weight of', -0.32978564940326077)\n",
      "('Worker_Class__Unemployeed', 'has weight of', -0.32607924970820285)\n",
      "('Education_Attained__no_high_school', 'has weight of', -0.31381084465277331)\n",
      "('Travel_Time__na', 'has weight of', -0.30977338159633672)\n",
      "('Marital_status__Never Married', 'has weight of', -0.24291596892372649)\n",
      "('Education_Attained__na', 'has weight of', -0.17876418187388157)\n",
      "('English__Not at all', 'has weight of', -0.1501971811103823)\n",
      "('English__Not well', 'has weight of', -0.14470121230967634)\n",
      "('Worker_Class__State Government', 'has weight of', -0.1087468002320595)\n",
      "('Education_Attained__some_college', 'has weight of', -0.053094833090232434)\n",
      "('Marital_status__Separated', 'has weight of', -0.043453441183022447)\n",
      "('Worker_Class__Local Government', 'has weight of', -0.039996795112791729)\n",
      "('English__Well', 'has weight of', -0.037800579599271411)\n",
      "('Miliary_Service__Serving Active Duty', 'has weight of', -0.034462983315286286)\n",
      "('Citizen__Non-Citizen', 'has weight of', -0.025394629743928223)\n",
      "('Citizen__US Territory Born', 'has weight of', -0.015536400054483963)\n",
      "('Miliary_Service__Served Not Active', 'has weight of', -0.0071799840485110948)\n",
      "('Citizen__US Born', 'has weight of', -0.0047461706833879201)\n",
      "('Worker_Class__Private Non-Profit', 'has weight of', -0.00013211418821038127)\n",
      "('Miliary_Service__Reserves', 'has weight of', 0.00010722163182343695)\n",
      "('Citizen__Born Abroad)', 'has weight of', 0.0026435686916035377)\n",
      "('Miliary_Service__Never Served', 'has weight of', 0.013592074272191897)\n",
      "('Marital_status__Divorced', 'has weight of', 0.025967850643647709)\n",
      "('PWGTP', 'has weight of', 0.031557597557460015)\n",
      "('Citizen__Naturalized', 'has weight of', 0.034100021515266608)\n",
      "('Worker_Class__Family Business - no pay', 'has weight of', 0.045448023811820934)\n",
      "('English__Very well', 'has weight of', 0.050077682454323161)\n",
      "('Travel_Time__short', 'has weight of', 0.054067056896112506)\n",
      "('PAP', 'has weight of', 0.056083120475836676)\n",
      "('English__Speaks only English', 'has weight of', 0.08889963407481058)\n",
      "('Marital_status__Widowed', 'has weight of', 0.090786994975403934)\n",
      "('Worker_Class__Federal Government', 'has weight of', 0.12651648036634558)\n",
      "('Travel_Time__long', 'has weight of', 0.15105608170346482)\n",
      "('Worker_Class__Self Employed (not incorportated)', 'has weight of', 0.15407266130916944)\n",
      "('Marital_status__Married', 'has weight of', 0.15814554547553877)\n",
      "('Travel_Time__hour', 'has weight of', 0.15948357217425541)\n",
      "('Travel_Time__half hour', 'has weight of', 0.16651842518465104)\n",
      "('Worker_Class__Self Employed (incorporated)', 'has weight of', 0.20061180470254561)\n",
      "('Worker_Class__Private for Profit', 'has weight of', 0.22696009876661297)\n",
      "('Education_Attained__doctorate_degree', 'has weight of', 0.29195778119550231)\n",
      "('Education_Attained__professional_degree', 'has weight of', 0.34312062773985991)\n",
      "('Education_Attained__bacholors_degree', 'has weight of', 0.45953069924709405)\n",
      "('Education_Attained__masters_degree', 'has weight of', 0.47434565710541987)\n",
      "('is_male', 'has weight of', 0.54662714060657525)\n",
      "('AGEP', 'has weight of', 0.61257094727957628)\n"
     ]
    }
   ],
   "source": [
    "def get_weights():\n",
    "   zip_vars = zip(lr_clf.coef_.T, lr2.columns) # combine attributes\n",
    "   zip_vars = sorted(zip_vars)\n",
    "   \n",
    "   print('\\nWeights of variables')\n",
    "   for coef, name in zip_vars:\n",
    "       print(name, 'has weight of', coef[0]) # now print them out\n",
    "\n",
    "get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SVM:', 0.78263750828363154)\n",
      "0:03:39.964482\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b6dbbed41bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tested_on\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wealthy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# add back in the 'Survived' Column to the pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mlr2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wealthy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;31m# also add it back in for the original data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "lr2 = lr.copy(deep=True)\n",
    "y = lr2.wealthy.values\n",
    "X = lr2.values\n",
    "\n",
    "svm_clf = SVC(C=0.5, class_weight='balanced')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_hat = svm_clf.predict(X_test)\n",
    "\n",
    "conf = mt.confusion_matrix(y_test, y_hat)\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "\n",
    "print('SVM:', acc)\n",
    "print (datetime.now() - startTime)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "# df_tested_on = pd.DataFrame(X_train) # saved from above, the indices chosen for training\n",
    "df_tested_on = lr2\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support.loc[:,'wealthy'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "lr2.loc[:,'wealthy'] = y # also add it back in for the original data\n",
    "df_support.info()\n",
    "\n",
    "\n",
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['wealthy'])\n",
    "df_grouped = lr2.groupby(['wealthy'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['AGEP']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['not_wealthy','wealthy'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['not_wealthy','wealthy'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
